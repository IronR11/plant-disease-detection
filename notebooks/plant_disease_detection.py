# -*- coding: utf-8 -*-
"""plant-disease-detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nt1nI6di2NdsbOjdBgAD-ed5Jfd4uusg
"""

from google.colab import drive
drive.mount('/content/drive')

from google.colab import drive
drive.mount('/content/drive')

import os, textwrap, pathlib

PROJ = "/content/drive/MyDrive/plant-disease-detection"
folders = [
    PROJ,
    f"{PROJ}/data",          # dataset lives here (unzipped)
    f"{PROJ}/models",        # saved models (.keras/.h5)
    f"{PROJ}/notebooks",     # your Colab notebooks (ipynb)
    f"{PROJ}/src",           # (optional) python modules later
    f"{PROJ}/outputs"        # (optional) predictions/plots
]
for d in folders:
    os.makedirs(d, exist_ok=True)

# create placeholder files so empty dirs stay visible
for keep in ["data", "models", "src", "outputs"]:
    pathlib.Path(f"{PROJ}/{keep}/.gitkeep").touch()

# minimal README & .gitignore (helpful when pushing to GitHub later)
readme = f"""\
# Plant Disease Detection (Colab + Kaggle API)

This project trains a CNN on PlantVillage-style leaf images to classify plant diseases.

## Structure
- `data/` — dataset (downloaded via Kaggle API, not committed)
- `models/` — trained models
- `notebooks/` — Colab notebooks
- `src/` — (optional) reusable python modules
- `outputs/` — predictions/plots

Run in Google Colab with GPU. Data is stored in Google Drive for persistence.
"""
with open(f"{PROJ}/README.md", "w", encoding="utf-8") as f:
    f.write(textwrap.dedent(readme))

gitignore = """\
# ignore large/local stuff
data/
models/
outputs/
.ipynb_checkpoints/
__pycache__/
*.pyc
"""
with open(f"{PROJ}/.gitignore", "w", encoding="utf-8") as f:
    f.write(gitignore)

print("Project root:", PROJ)
print("Contents:", os.listdir(PROJ))

import os
PROJ = "/content/drive/MyDrive/plant-disease-detection"
os.chdir(PROJ)
!pwd
!ls -la

from google.colab import files
files.upload()   # select kaggle.json

import os, shutil

# create folder if not exists
os.makedirs("/root/.kaggle", exist_ok=True)

# move the uploaded kaggle.json to ~/.kaggle/
shutil.move("kaggle.json", "/root/.kaggle/kaggle.json")

# set correct permissions
os.chmod("/root/.kaggle/kaggle.json", 600)

!kaggle datasets list -s plant

# make sure we’re in project data folder
DATA_PATH = "/content/drive/MyDrive/plant-disease-detection/data"
os.chdir(DATA_PATH)

# download + unzip
!kaggle datasets download -d emmarex/plantdisease -p {DATA_PATH}
!unzip -q plantdisease.zip -d plantvillage
!rm plantdisease.zip

# check folder
!ls plantvillage | head

"""# Start From Here

"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import matplotlib.pyplot as plt
import pathlib

# Path where dataset is stored
data_dir = pathlib.Path("/content/drive/MyDrive/plant-disease-detection/data/plantvillage/PlantVillage")

# check number of images
image_count = len(list(data_dir.glob('*/*.jpg')))
print("Total images:", image_count)

batch_size = 32
img_height = 180
img_width = 180

train_ds = tf.keras.utils.image_dataset_from_directory(
    data_dir,
    validation_split=0.2,   # 80% training, 20% validation
    subset="training",
    seed=123,
    image_size=(img_height, img_width),
    batch_size=batch_size
)

val_ds = tf.keras.utils.image_dataset_from_directory(
    data_dir,
    validation_split=0.2,
    subset="validation",
    seed=123,
    image_size=(img_height, img_width),
    batch_size=batch_size
)

class_names = train_ds.class_names
print("Class Names:", class_names)

AUTOTUNE = tf.data.AUTOTUNE

train_ds = train_ds.shuffle(1000).prefetch(buffer_size=AUTOTUNE)
val_ds = val_ds.prefetch(buffer_size=AUTOTUNE)

normalization_layer = layers.Rescaling(1./255)

normalized_train_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))
normalized_val_ds = val_ds.map(lambda x, y: (normalization_layer(x), y))

from tensorflow import keras
from tensorflow.keras import layers

data_augmentation = keras.Sequential(
    [
        layers.RandomFlip("horizontal"),   # flip images left-to-right
        layers.RandomRotation(0.1),        # rotate images slightly
        layers.RandomZoom(0.2),            # zoom randomly
    ]
)

from tensorflow import keras
from tensorflow.keras import layers

num_classes = len(class_names)  # total number of plant disease categories

model = keras.Sequential([
    data_augmentation,   # Step 5: Apply augmentation

    layers.Conv2D(32, 3, activation='relu'),
    layers.MaxPooling2D(),

    layers.Conv2D(64, 3, activation='relu'),
    layers.MaxPooling2D(),

    layers.Conv2D(128, 3, activation='relu'),
    layers.MaxPooling2D(),

    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.5),   # prevent overfitting
    layers.Dense(num_classes, activation='softmax')  # output layer
])

model.compile(
    optimizer='adam',  # efficient optimizer
    loss='sparse_categorical_crossentropy',  # good for integer labels
    metrics=['accuracy']  # track accuracy during training
)

history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=1  # you can increase later, start small for testing
)

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import matplotlib.pyplot as plt

img_size = (224, 224)
batch_size = 32

train_ds = keras.utils.image_dataset_from_directory(
    data_dir,
    validation_split=0.2,
    subset="training",
    seed=123,
    image_size=img_size,
    batch_size=batch_size,
)

val_ds = keras.utils.image_dataset_from_directory(
    data_dir,
    validation_split=0.2,
    subset="validation",
    seed=123,
    image_size=img_size,
    batch_size=batch_size,
)

class_names = train_ds.class_names
num_classes = len(class_names)
print("Class names:", class_names)

AUTOTUNE = tf.data.AUTOTUNE
train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)
val_ds = val_ds.prefetch(buffer_size=AUTOTUNE)

base_model = keras.applications.MobileNetV2(
    input_shape=(224, 224, 3),
    include_top=False,   # remove final classification head
    weights="imagenet"   # use pre-trained ImageNet weights
)

base_model.trainable = False  # freeze weights so they don’t change

inputs = keras.Input(shape=(224, 224, 3))
x = base_model(inputs, training=False)  # pass images through base model
x = layers.GlobalAveragePooling2D()(x) # reduce features
x = layers.Dropout(0.2)(x)              # prevent overfitting
outputs = layers.Dense(num_classes, activation="softmax")(x)

model = keras.Model(inputs, outputs)

model.compile(
    optimizer=keras.optimizers.Adam(),
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"]
)

history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=10
)

model.save("plant_disease_transfer.h5")

for images, labels in train_ds.take(1):
    print("Train batch shape:", images.shape)
    print("Label batch shape:", labels.shape)

for images, labels in val_ds.take(1):
    print("Val batch shape:", images.shape)
    print("Label batch shape:", labels.shape)

model.summary()

images, labels = next(iter(train_ds))
preds = model(images[:1])   # run 1 image through model
print("Prediction shape:", preds.shape)

from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

# Stop training if validation accuracy doesn’t improve for 3 epochs
early_stopping = EarlyStopping(
    monitor="val_accuracy",
    patience=3,
    restore_best_weights=True
)

# Save the best model
checkpoint = ModelCheckpoint(
    "best_transfer_model.keras",
    monitor="val_accuracy",
    save_best_only=True
)

history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=50,   # start small, you can increase later
    callbacks=[early_stopping, checkpoint]
)

import matplotlib.pyplot as plt

acc = history.history["accuracy"]
val_acc = history.history["val_accuracy"]
loss = history.history["loss"]
val_loss = history.history["val_loss"]

epochs_range = range(len(acc))

plt.figure(figsize=(12, 6))

# Accuracy
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label="Training Accuracy")
plt.plot(epochs_range, val_acc, label="Validation Accuracy")
plt.legend(loc="lower right")
plt.title("Training vs Validation Accuracy")

# Loss
plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label="Training Loss")
plt.plot(epochs_range, val_loss, label="Validation Loss")
plt.legend(loc="upper right")
plt.title("Training vs Validation Loss")

plt.show()

val_loss, val_acc = model.evaluate(val_ds)
print(f"Validation Accuracy: {val_acc:.2f}")
print(f"Validation Loss: {val_loss:.2f}")

import numpy as np
from sklearn.metrics import classification_report, confusion_matrix

# True labels
y_true = np.concatenate([y for x, y in val_ds], axis=0)

# Predictions
y_pred_probs = model.predict(val_ds)
y_pred = np.argmax(y_pred_probs, axis=1)

print(classification_report(y_true, y_pred, target_names=class_names))

import seaborn as sns
import matplotlib.pyplot as plt

cm = confusion_matrix(y_true, y_pred)

plt.figure(figsize=(12, 8))
sns.heatmap(cm, annot=False, cmap="Blues", xticklabels=class_names, yticklabels=class_names)
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix")
plt.show()

model.save("plant_disease_model.h5")  # saves in HDF5 format

from google.colab import files
files.download("plant_disease_model.h5")

